{"cells":[{"cell_type":"markdown","source":["##### Converting the predictions at day level."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1fc28970-aab5-4569-85c9-e253aee401f0"}}},{"cell_type":"code","source":["def day_level_result(Asset_list,Raw_data_path,RN,NR,features_to_test,model_path,Anomaly_threshold):\n  \n  '''This function loads the predictions and then aggregate the trace level results into day level. Anomaly threshold column is created which tells the       percentage of traces marked as anomalous in a particular day. Finally, based on Anomaly_threshold any day is either marked as anomalous or             normal\n  \n  Input :: Pandas dataframe containing the predictions of random forest against all the traces either 0 or 1. Anomaly_threshold is also the input.\n  Output :: Pandas dataframe containing the aggregated predictions of Random forest at day level and  prediction based on Anomaly threshold.\n  '''\n  \n  model_result = predictions_at_trace_level(Asset_list,Raw_data_path,RN,NR,features_to_test,model_path)\n  ### Calling the predictions_at_trace_level function from the previous file\n  \n  data_iso_forest_withCount =  model_result.groupby(['Name','DATE','forest_Predictions']).forest_Predictions.agg('count').to_frame('Anomaly_Count').reset_index()\n  ## Grouping and aggregating the trace level result and creating anomaly_count column\n  \n  Result_df_noanomaly = data_iso_forest_withCount[data_iso_forest_withCount.forest_Predictions ==1]\n  ## Selecting only the count of normal traces in a day\n  \n  noanomaly =Result_df_noanomaly[['Name','DATE','Anomaly_Count']]\n  noanomaly.columns =['Name','DATE','NoAnomaly_Count']\n\n  Result_df_anomaly = data_iso_forest_withCount[data_iso_forest_withCount.forest_Predictions==0]\n  ## Selecting only the count of anomalous traces in a day\n  anomaly =Result_df_anomaly[['Name','DATE','Anomaly_Count']]\n  anomaly.columns =['Name','DATE','YesAnomaly_Count']\n\n  Temp =data_iso_forest_withCount.merge(anomaly, on = ['Name','DATE'],how='left')\n  Result_df_final =Temp.merge(noanomaly, on = ['Name','DATE'],how='left')\n  ## Joining the above two dataframes to get anomalous trace count and normal trace count against each date\n  \n  Result_df_final.YesAnomaly_Count.fillna(0, inplace=True)\n  Result_df_final.NoAnomaly_Count.fillna(0, inplace=True)\n  ## Replacing the NA's with 0\n  \n  Result_df_final['Anomaly_percentage'] = (((Result_df_final['YesAnomaly_Count'])/(Result_df_final['YesAnomaly_Count']+Result_df_final['NoAnomaly_Count'])))*100\n  ### Calculating the Percentage of anomalous traces for each day.\n\n  Result_df_final =Result_df_final.drop_duplicates(['Name','DATE','YesAnomaly_Count','NoAnomaly_Count'])\n  ### Dropping duplicates\n  \n  Result_df_final = Result_df_final.drop(columns = ['forest_Predictions','Anomaly_Count'])\n  ### Dropping extra columns\n  \n  Result_df_final = Result_df_final.sort_values(['Name','DATE'])\n  ## Sorting based on Asset name and Date\n  \n  \n  ### Creating a new column y based on Anomaly_percentage. 1 means the day has been marked as anomalous and 0 means the day has been marked as normal\n  if Anomaly_threshold == 0.0:\n\n      # For 0 Anamoly Percentage\n\n      Result_df_final.loc[Result_df_final['YesAnomaly_Count'] >= 1.0, 'y'] = 1\n\n      Result_df_final['y'] = Result_df_final['y'].fillna(0)\n\n      #For Anamoly Percentage\n\n  else:\n\n      Result_df_final.loc[Result_df_final['Anomaly_percentage'] >= Anomaly_threshold, 'y'] = 1\n\n      Result_df_final['y'] = Result_df_final['y'].fillna(0)\n  \n  return Result_df_final\n  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7da9c46-9836-4005-8c77-7f95f7cb3666"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"4.Convert_Prediction_To_Day_Level","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2700526048032585}},"nbformat":4,"nbformat_minor":0}
