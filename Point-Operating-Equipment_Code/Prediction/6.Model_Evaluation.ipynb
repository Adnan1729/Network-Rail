{"cells":[{"cell_type":"code","source":["def model_evaluation(result_path):\n  result = pd.read_csv(result_path)\n  result = result.drop(columns = ['Unnamed: 0'])\n  result['DATE'] = pd.to_datetime(result['DATE'])\n  Asset_list = pd.read_csv(\"/dbfs/FileStore/shared_uploads/asingha3@networkrail.co.uk/RADAR_Total_Asset_List.csv\")\n  Asset_list = Asset_list[[\"EllipseNo\",\"AssetName\"]]\n  Asset_list[\"EllipseNo\"] = Asset_list[\"EllipseNo\"].astype(\"str\").str.pad(width=12, side='left', fillchar='0')\n  Asset_list.rename(columns = {'EllipseNo':\"EllipseID\"},inplace=True)\n  Asset_list.rename(columns = {'AssetName':\"Name\"},inplace=True)\n  result = result.merge(Asset_list,how='left',on=['Name'])\n  Loggers_list = result.EllipseID.unique().tolist()\n  \n  FMS_Faults_clamplockph3= FMS_Faults = spark.read.jdbc(url=innvUrl, table='dbo.tbl_cf10_AssetFMS_InDT', properties=properties)\n  FMS_Faults_clamplockph3 = FMS_Faults_clamplockph3.filter(FMS_Faults_clamplockph3.EQUIP_NO.isin(Loggers_list))\n  FMS_Faults_clamplockph3 = FMS_Faults_clamplockph3.select(['EQUIP_NO', 'COMPONENT_LEVEL_1','FAILED_DT','FAULT_RESOLVED_DT','InOrderDate','COMPONENT_LEVEL_2','FAILURE_NUMBER','D_FAILURE_KEY'])\n  FMS_Faults_clamplockph3 = FMS_Faults_clamplockph3.withColumn('InOrderDate' , fn.to_date(fn.col('InOrderDate'),format =\"YYYY-MM-ddTHH:MM:SS.sss+ssss\"))\n  FMS_Faults_clamplockph3 = FMS_Faults_clamplockph3.withColumn('FAILED_DT',fn.to_date(fn.col('FAILED_DT'),format =\"YYYY-MM-ddTHH:MM:SS.sss+ssss\"))\n  FMS_Faults_clamplockph3 = FMS_Faults_clamplockph3.withColumn('FAULT_RESOLVED_DT',fn.to_date(fn.col('FAULT_RESOLVED_DT'),format =\"YYYY-MM-ddTHH:MM:SS.sss+ssss\"))\n  \n  WO = spark.read.jdbc(url=jdbcUrl, table='DST.v_cf10_AssetWorkorder', properties=connectionProperties)\n\n  WO = WO.filter(WO.equip_no.isin(Loggers_list))\n\n  #display(WO)\n\n  #WO = WO.filter(WO.EQUIP_NO.isin(loggers_list))Â \n\n  Work_Order_clamplockph3 =WO.select('equip_no','WO_CLOSED_DATE','WORK_ORDER_STATUS','WORK_ORDER_DESC')\n\n  Work_Order_clamplockph3 = Work_Order_clamplockph3[Work_Order_clamplockph3['WORK_ORDER_STATUS']=='Closed']\n\n  Work_Order_clamplockph3 =Work_Order_clamplockph3[['equip_no','WO_CLOSED_DATE','WORK_ORDER_DESC']]\n\n  old_column = ['equip_no','WO_CLOSED_DATE','WORK_ORDER_DESC']\n\n  newcolumns =['EQUIP_NO','WO_DATE','WORK_ORDER_DESC']\n\n  for i in range(0,2):\n    Work_Order_clamplockph3 = Work_Order_clamplockph3.withColumnRenamed(old_column[i],newcolumns[i])\n    \n  Faults = FMS_Faults_clamplockph3.toPandas()\n  WO = Work_Order_clamplockph3.toPandas()\n  WO = WO.rename(columns = {\"WO_DATE\":\"WO_CLOSED_DATE\"})\n  WO = WO.rename(columns = {\"EQUIP_NO\":\"EllipseID\"})\n  WO[\"Impact_on_POE\"] = 1\n  Faults = Faults.rename(columns = {\"EQUIP_NO\":\"EllipseID\"})\n  \n  Result_sdf = spark.createDataFrame(result)\n  Faults['FAILED_DT'] = pd.to_datetime(Faults['FAILED_DT'])\n  Faults['FAULT_RESOLVED_DT'] = pd.to_datetime(Faults['FAULT_RESOLVED_DT'])\n\n  Faults['InOrderDate'] = pd.to_datetime(Faults['InOrderDate'])\n\n  WO['WO_CLOSED_DATE'] = pd.to_datetime(WO['WO_CLOSED_DATE'])\n  \n  Result_sdf = Result_sdf.withColumn('DATE',fn.to_date(fn.col('DATE'),format=\"yyy-MM-DDTHH:MM:SS.sss+sss\"))\n\n  Result_sdf = Result_sdf.dropDuplicates()\n\n  new_WO = WO.groupby([\"EllipseID\",\"WO_CLOSED_DATE\"], as_index = False)[\"Impact_on_POE\"].sum()\n\n  new_WO = new_WO[new_WO[\"Impact_on_POE\"] > 0]\n\n  new_WO[\"EllipseID\"] = new_WO[\"EllipseID\"].astype(\"str\").str.pad(width=12, side='left', fillchar='0')\n\n  new_WO.rename(columns = {'EllipseID':'EQUIP_NO'},inplace=True)\n\n  Faults[\"EllipseID\"] = Faults[\"EllipseID\"].astype(\"str\").str.pad(width=12, side='left', fillchar='0')\n\n  new_FMS = Faults.copy()\n\n  new_FMS.rename(columns ={\"EllipseID\":\"EQUIP_NO\"},inplace = True)\n\n  new_WO = spark.createDataFrame(new_WO)\n\n  new_FMS = spark.createDataFrame(new_FMS)\n\n  new_WO = new_WO.withColumn('WO_CLOSED_DATE',fn.to_date(fn.col('WO_CLOSED_DATE')))\n\n  new_FMS = new_FMS.withColumn('FAILED_DT',fn.to_date(fn.col('FAILED_DT')))\n\n  new_FMS = new_FMS.withColumn('FAULT_RESOLVED_DT',fn.to_date(fn.col('FAULT_RESOLVED_DT')))\n\n  new_FMS = new_FMS.withColumn('InOrderDate',fn.to_date(fn.col('InOrderDate')))\n\n  work_order = new_WO.withColumn('prev_dt_WO',fn.lag(fn.col('WO_CLOSED_DATE')).over(Window.partitionBy('EQUIP_NO').orderBy('WO_CLOSED_DATE')))\n\n  default_date= '2017-01-01'\n\n  work_order =work_order.withColumn('prev_dt_WO',fn.coalesce(fn.col('prev_dt_WO'),fn.lit(default_date)))\n\n  work_order=work_order.dropDuplicates(['equip_no','WO_CLOSED_DATE'])\n\n  Result_WO = Result_sdf.join(work_order,(Result_sdf['EllipseID']==work_order['EQUIP_NO'])&(Result_sdf['DATE']>work_order['prev_dt_WO'])&(Result_sdf['DATE']<=work_order['WO_CLOSED_DATE']),how='left')\n\n  Result_WO = Result_WO.withColumn('Day_before_WO',fn.datediff('WO_CLOSED_DATE','DATE'))\n\n  faults = new_FMS\n\n  faults = faults.withColumn('prev_dt_fault',fn.lag(fn.col('FAILED_DT')).over(Window.partitionBy('EQUIP_NO').orderBy('FAILED_DT')))\n\n  default_date= '2017-01-01'\n\n  faults =faults.withColumn('prev_dt_fault',fn.coalesce(fn.col('prev_dt_fault'),fn.lit(default_date)))\n\n  faults=faults.dropDuplicates(['EQUIP_NO','FAILED_DT'])\n\n  result_fault_WO = Result_WO.join(faults,(Result_WO['EllipseID']==faults['EQUIP_NO'])&(Result_WO['DATE']>faults['prev_dt_fault'])&(Result_WO['DATE']<=faults['FAILED_DT']),how='left')\n\n  result_fault_WO = result_fault_WO.withColumn('Day_before_fault',fn.datediff('FAILED_DT','DATE'))\n\n  equip_date = result_fault_WO.select('EllipseID','DATE')\n\n  faults = faults.withColumn('next_fault_dt',fn.lag(fn.col('FAILED_DT'),-1).over(Window.partitionBy('EQUIP_NO').orderBy('FAILED_DT')))\n\n  resolving_day1 = equip_date.join(faults,(equip_date['EllipseID']==faults['EQUIP_NO'])&(equip_date['DATE']>faults['FAILED_DT'])&(equip_date['DATE']<=faults['InOrderDate'])&(equip_date['DATE']<faults['next_fault_dt']),how='left')\n\n  resolving_day = resolving_day1.withColumn('resolving_day',fn.when(fn.col('InOrderDate').isNotNull(),fn.lit('y')).otherwise('n'))\n\n  resolving_day =resolving_day.select('EllipseID','DATE','resolving_day')\n\n  result_fault_WO = result_fault_WO.sort('EllipseID','DATE')\n\n  feat_all = result_fault_WO.join(resolving_day,['EllipseID','DATE'],how='left')\n\n  feat_all =feat_all.drop('EQUIP_NO','prev_dt')\n\n  feat_all = feat_all.dropDuplicates()\n\n  Result_sdf= feat_all\n\n  Result_df = Result_sdf.toPandas()\n  \n  Results = Result_df.copy()\n  \n  Results['Day_before_WO'] = Results['Day_before_WO'].fillna(999)\n\n  Results['Day_before_fault'] = Results['Day_before_fault'].fillna(999)\n\n  Results_TP = Results.loc[(Results['Day_before_WO']>0)&(Results['Day_before_fault']>0)&(Results['Day_before_fault']<8)]\n\n\n  Total_failures = Results_TP.FAILURE_NUMBER.nunique()\n  #can be changed\n\n  TP_num = Results_TP.loc[Results_TP['final_prediction']==0].FAILURE_NUMBER.nunique()\n\n  TP = (TP_num/Total_failures)*100\n\n  Results_FP = Results.loc[(Results['Day_before_WO']>=8)&(Results['Day_before_fault']>=8)]\n\n\n  Results_FP_den = Results_FP.shape[0]\n\n  Results_FP_num = Results_FP.loc[Results_FP['final_prediction']==0].shape[0]\n\n  FP = (Results_FP_num/Results_FP_den)*100\n\n  return(print(\"TP {}/{}->{:.2f}%   FP {}/{}->{:.2f}%\".format(TP_num,Total_failures,TP,Results_FP_num,Results_FP_den,FP)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9a644a4-7ac0-46f0-898b-91d2dffc1f2d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["model_evaluation(result_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97f741ef-319c-4677-ad0e-8fed511ab3c8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/databricks/spark/python/pyspark/sql/pandas/utils.py:81: UserWarning: The conversion of DecimalType columns is inefficient and may take a long time. Column names: [D_FAILURE_KEY] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n  &#34;conversion.&#34;.format(&#34;, &#34;.join(decimal_col_names))\nTP 21/43-&gt;48.84%   FP 497/5083-&gt;9.78%\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/sql/pandas/utils.py:81: UserWarning: The conversion of DecimalType columns is inefficient and may take a long time. Column names: [D_FAILURE_KEY] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n  &#34;conversion.&#34;.format(&#34;, &#34;.join(decimal_col_names))\nTP 21/43-&gt;48.84%   FP 497/5083-&gt;9.78%\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"6.Model_Evaluation","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2700526048032597}},"nbformat":4,"nbformat_minor":0}
