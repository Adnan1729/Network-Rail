{"cells":[{"cell_type":"markdown","source":["##### Loading raw data and segmenting the traces."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b078a085-8712-4207-a8ac-ecb2860fc8e5"}}},{"cell_type":"code","source":["def load_data_segment_traces(Asset_list,Raw_data_path,RN,NR):\n  \n  '''This function takes Asset_list as argument which is its input, it will load the raw wonderware data for these Assets ,construct the traces and mark the three phases of the traces.\n  Input :: Name of Assets in a list format, Path where the wonderware data currently stored, Trace directions NR and RN\n  Output :: Pyspark data frame with segmented traces'''\n  \n  #### Loading parquet data from mounted folder \n  df = spark.read.parquet(Raw_data_path).filter((col('Attribute') == RN)|(col('Attribute') == NR)).where((col('Asset').isin(Asset_list)))\n  ## VALUE column contains the current readings in Amperes\n  df = df.withColumnRenamed(\"Asset\", \"Name\")\\\n       .withColumnRenamed(\"Value\", \"VALUE\")\n  \n  \n  ## Renaming the columns\n  df = df.withColumn('TraceDir', fn.substring('Attribute', -2,2))\n  \n  \n  # Getting TraceDirection from Attribute, here NR means 'Normal to reverse' and RN means 'Reverse to Normal'\n  columns_to_drop =['Attribute', 'Quality','Day','Month','Year']\n  df = df.drop(*columns_to_drop)\n  \n  \n  ## Dropping Extra columns\n  TimeFormat = \"dd/MM/yyyy' 'HH:mm:ss.SSS\"\n  df = df.withColumn('DateTime2', fn.unix_timestamp('DateTime', TimeFormat) + fn.substring('DateTime', -3,3).cast('float')/1000 )\n  \n  \n  ## Converting the datetime stamp to UNIX (EPOCH) timestamp\n  df = df.withColumn('DateTime3', fn.from_unixtime('DateTime2').cast(DateType()))\n  \n  \n  ## Extracting only dates from Datetime stamp\n  df = df.withColumn(\"DateTime4\",fn.to_timestamp(fn.col(\"DateTime2\")))\n  df = df.withColumn(\"VALUE\", df[\"VALUE\"].cast(DoubleType()))\n  ## Casting the VALUE column to double type\n  \n  #############################################################################################################################################    \n  ##### Trace segmentation from raw data for each Asset and marking TraceIds\n  \n  TimeFormat = \"yyyy/MM/dd' 'HH:mm:ss.SSS\"\n  # Calculating the difference of timestamp with previous timestamp\n  df = df.withColumn(\"difftime\",(df.DateTime2 - lag(df.DateTime2,1)\\\n                            .over(Window.partitionBy(\"Name\", \"TraceDir\")\\\n                                  .orderBy(\"DateTime2\")))).na.fill(0)\n\n  # Calculating the difference of current(in Amperes) value with previous current Value\n  df = df.withColumn(\"diffval\",(df.VALUE - lag(df.VALUE,1)\\\n                            .over(Window.partitionBy(\"Name\", \"TraceDir\")\\\n                                  .orderBy(\"DateTime2\")))).na.fill(0)\n\n  # TraceNum increases every time two consecutive readings is 0 or timegap between two readings exceeds 5 seconds \n  df = df.withColumn(\"isgap\", ((df.difftime > 5)|(df.difftime < -5)) | ((df.VALUE == 0)&(df.diffval == 0)))\n\n  df = df.withColumn(\"TraceNum\", fn.sum(fn.col(\"isgap\").cast(\"long\")).over(Window.partitionBy(\"Name\")\\\n                                  .orderBy(\"DateTime2\"))) \n\n  win = Window.partitionBy(\"Name\", \"TraceDir\", \"TraceNum\").orderBy(\"DateTime2\")\n  df = df.withColumn(\"PosIndx\", rank().over(win))\n\n  win = Window.partitionBy(\"Name\", \"TraceDir\", \"TraceNum\")\\\n              .orderBy(\"DateTime2\")\\\n              .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n\n  ## PosIndx gives the total swing time or number of datapoints in a trace\n  df = df.withColumn(\"MaxIndx\", fn.max(\"PosIndx\").over(win))\n\n\n  ## This is segmentation logic here 1st 25% of the datapoints are u/\n\n  df = df.withColumn(\"Phase\",\\\n  (when(col(\"PosIndx\") <= col('MaxIndx')/4, 0)\\\n  .when(col(\"PosIndx\") <= 3*col('MaxIndx')/4, 1)\\\n  .otherwise(2)))\n  \n  ## This is to filter the traces which have swingtime less than 150 and ignore traces from midnight to 5 A.M\n  df = df.withColumn('hour',fn.hour('DateTime4'))\n  df = df.filter((fn.col('hour') >=5) & (fn.col('MaxIndx') >= 150))\n  \n  \n  return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9628c481-2a3b-426e-91f7-3bf86f8eb299"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91f122b2-67f6-42ec-9678-0dbe922622f4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1267151654466348&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>df\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;df&#39; is not defined</div>","errorSummary":"<span class=\"ansi-red-fg\">NameError</span>: name &#39;df&#39; is not defined","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1267151654466348&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>df\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;df&#39; is not defined</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d78290a-ffeb-4d30-a0c3-2f3198fa9333"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"1.Load_Data_And_Construct_Traces","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2700526048032588}},"nbformat":4,"nbformat_minor":0}
